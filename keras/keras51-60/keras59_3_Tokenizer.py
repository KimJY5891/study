from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
text01 = '나는 진짜 매우 매우 맛잇는 밥을 엄청 마구 마구 마구 먹었다.'
# 띄어 쓰기 잖아 음절
# 컴퓨터가 인식하기 위해서는 수치화 
# 텍스트도 인식하게 하기 위해서 수치화
text02 = '나는 지구용사 배환희다. 멋있다. 또 또 얘기해부아'


token = Tokenizer()
# 전처리 
token.fit_on_texts([text01,text02]) 
# 문장이 여러 문장이 있을 수 있다. 그럴 경우 리스트
print(token.word_index) 
#{'마구': 1, '나는': 2, '매우': 3, '또': 4, '진짜': 5, '맛잇는': 6, '밥을': 7, '엄청': 8, '먹었다': 9, '지구용사': 10, '배환희다': 11, '멋있다': 12, '얘기해부아': 13}
# 키밸류 형태
# 중복된 건 또 작성은 안되어있음 
# word_index 글을 인덱싱 
# 가장 많은 애가 가장 앞으로 가있다.
print(token.word_counts)
# 키밸류 형태
# OrderedDict([('나는', 2), ('진짜', 1), ('매우', 2), ('맛잇는', 1), ('밥을', 1), ('엄청', 1), ('마구', 3), ('먹었다', 1), ('지구용사', 1), ('배환희다', 1), ('멋있다', 1), ('또', 2), ('얘기해부아', 1)])
# 단어가 나온 형태 

x = token.texts_to_sequences(
     [text01,text02]
) # 여러 개있을 수 도 잇기 때문에 리스트 형태로 작성한다.
# 숫자로 바꿔주는 작업
# 결과를 변수(메모리)에 넣어준다. 
print(x) 
#[[2, 5, 3, 3, 6, 7, 8, 1, 1, 1, 9], [2, 10, 11, 12, 4, 4, 13]]
# 길이가 다른건 나중에 0을 채워서 맞춰준다. 
print(np.array(x).shape) # (1, 11)
# 그냥 연산하면 숫자에 따라서 가치가 높다고 생각하게 된다. 
# 위에 수치는 가치가 다른 것이 아니라 평등 관게이다. 
# 가치가 같으니까 =>  원핫 인코딩을 해줘야한다. 
######################## 1. to_categorical ########################
x = x[0] + x[1]
print(x) #[2, 5, 3, 3, 6, 7, 8, 1, 1, 1, 9, 2, 10, 11, 12, 4, 4, 13]
# 두 개의 리스트를 하나로
# from tensorflow.keras.utils import to_categorical
# x = np.array(x)
# x  = to_categorical(x)
# print(x)
# [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
# [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
#  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
#  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
#  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
# print(x.shape) #(18, 14)
# (1, 11, 8)(1~8)이 아닌 이유 - to 카테고리 컬은 무조건 0부터 시작함
# 그래서 필요 없는 0 있음
# 0을 지워주고 리쉐이프 해줘야함

# 넘파이 콘케 어쩌구로 이어보기 정리
######################## 2. get_dummies ########################
import pandas as pd  
# x = pd.get_dummies(x) # 1차원으로 받아들여야한다. 
# print(x)
# Traceback (most recent call last):
#   File "c:\study\keras\keras59_Tokenizer.py", line 50, in <module>
#     print(x.shape) #(1, 11, 9)
# AttributeError: 'list' object has no attribute 'shape'
# 겟더미에서 리스트 형태를 받아들일 수 없다. 
# 1. 넘파이
# x = np.array(x)
# x = 플랫튼과 동일 x.ravel()
# x = x.flatten()
# x = pd.get_dummies(x)
print(x.shape)
x = np.array(x)
x = x.reshape(-1,)
x = pd.get_dummies(x)
print(x.shape)
# 2. 왜 리스트를 받아들이지 못할까?
# sklearn.preprocessing.OneHotEncoder를 사용하여 
# 변환된 결과는 numpy.array이기 때문에
# 이를 데이터프레임으로 변환하는 과정이 필요하다.
'''


######################## 3. 사이킷런 onehot ########################
from sklearn.preprocessing import OneHotEncoder # 2차원으로 받아들여야한다. 
# 알아서 만들자
ohe = OneHotEncoder(sparse=False)
x = np.array(x)
x = x.reshape(-1,1)
print(x)
print(x.shape) # 가로로 값이 있느넥 아니라 # 변환 할 때 세로로 변환 시켜서 0010000 이런 형태로 바궈줘야한다. 
x = ohe.fit_transform(x)
print(x)
print(x.shape)


'''
