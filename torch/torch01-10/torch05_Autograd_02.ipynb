{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 자동 미분 흐름 예제\n",
        "*    계산 흐름 a -> b -> c -> out\n",
        "미분 out / 미분 a  = ???\n",
        "*    backward()를 통해 a <- b <- c <- out을 계산하면  미분 out / 미분 a 값이 a.gard에 채워짐"
      ],
      "metadata": {
        "id": "iLogkjhi-b87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.ones(2,2,requires_grad=True)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO9lSzd1-gWs",
        "outputId": "c527b75d-01cb-4b1a-8275-62123c56757a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.data)\n",
        "print(a.grad) # 텐서에 들어간 연산이 없고, 초기화된 상태일 뿐이다. 그래서 none이 나온다.\n",
        "print(a.grad_fn) # 텐서에 들어간 연산이 없고, 초기화된 상태일 뿐이다. 그래서 none이 나온다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ELghLPTCSeF",
        "outputId": "b78a397a-c7e2-4060-d198-89c4955d9b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b = a + 2"
      ],
      "metadata": {
        "id": "i6iJpON4Ck9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = a + 2\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is4GB14DCllS",
        "outputId": "118d582c-624a-457c-bdf3-835d4b271eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c = b **2"
      ],
      "metadata": {
        "id": "NOwThiQkDA34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = b ** 2\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osn-AOe8DBEr",
        "outputId": "6dd3d6be-3a98-4e3a-9b9f-192091aec506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9., 9.],\n",
            "        [9., 9.]], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = c.sum()\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLzebdA6DDkz",
        "outputId": "09e79f9a-106f-47bd-c0f6-438ec0e5136f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out)\n",
        "out.backward() # out으로 부터 백워드 진행"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPAiDPbaDO4h",
        "outputId": "1b165f28-3516-49c9-cdb5-351cbc1bef7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a의 grad_fn이 None인 이유는 직접적으로 계산한 부분이 없었기 때문"
      ],
      "metadata": {
        "id": "47h2jV6hDn6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.data)\n",
        "print(a.grad)\n",
        "print(a.grad_fn) # a를 직접적으로 계산하고 반영된 것은 없고 a를 활용해서 값을 도출한 것들만 있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hno4wtjYDnLG",
        "outputId": "37151a5d-5ae1-403a-db40-6091234c122c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[6., 6.],\n",
            "        [6., 6.]])\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b.data)\n",
        "print(b.grad)\n",
        "print(b.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dan0dJJGEN0y",
        "outputId": "e0d43917-1a64-4fbe-ca5a-f53183870d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]])\n",
            "None\n",
            "<AddBackward0 object at 0x7e7f1e40f160>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-1f70bb019fb2>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print(b.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(c.data)\n",
        "print(c.grad)\n",
        "print(c.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWRvtwrzEN9M",
        "outputId": "ac7537ba-1363-4d64-e3d3-8209fe1aac0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9., 9.],\n",
            "        [9., 9.]])\n",
            "None\n",
            "<PowBackward0 object at 0x7e7f0df5ac20>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-1dc4d8a383bf>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print(c.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.data)\n",
        "print(out.grad)\n",
        "print(out.grad_fn)\n",
        "# <SumBackward0 object at 0x7e7f0d0c3eb0>가 그래디언트 펑션으로 들어가는 것을 알 수가 있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh8grrbKEODq",
        "outputId": "2aaa4921-746a-4532-cdb0-a5812e6428c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36.)\n",
            "None\n",
            "<SumBackward0 object at 0x7e7f0d0c3eb0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-dbb015aaffa0>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print(out.grad)\n"
          ]
        }
      ]
    }
  ]
}